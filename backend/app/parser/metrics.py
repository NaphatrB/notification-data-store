"""Prometheus metrics for the parser service."""

import logging
import threading

from prometheus_client import Counter, Gauge, Histogram, start_http_server

logger = logging.getLogger(__name__)

METRICS_PORT = 9090

# ---------------------------------------------------------------------------
# Pipeline counters
# ---------------------------------------------------------------------------

processed_total = Counter(
    "parser_processed_total",
    "Total number of events processed by the parser",
)

failed_total = Counter(
    "parser_failed_total",
    "Total number of events that failed parsing (LLM or validation error)",
)

dead_letter_total = Counter(
    "parser_dead_letter_total",
    "Total number of events sent to dead-letter table",
)

# ---------------------------------------------------------------------------
# LLM call metrics
# ---------------------------------------------------------------------------

llm_call_total = Counter(
    "parser_llm_call_total",
    "Total number of LLM API calls made",
)

llm_error_total = Counter(
    "parser_llm_error_total",
    "Total number of LLM calls that failed (HTTP or JSON parse error)",
)

llm_call_duration = Histogram(
    "parser_llm_call_duration_seconds",
    "Duration of individual LLM API calls",
    buckets=[1, 2, 5, 10, 20, 30, 45, 60, 90, 120],
)

llm_prompt_tokens = Counter(
    "parser_llm_prompt_tokens_total",
    "Total prompt/input tokens consumed by LLM calls",
)

llm_completion_tokens = Counter(
    "parser_llm_completion_tokens_total",
    "Total completion/output tokens generated by LLM calls",
)

# ---------------------------------------------------------------------------
# Batch + age metrics
# ---------------------------------------------------------------------------

batch_latency = Histogram(
    "parser_batch_latency_seconds",
    "Time taken to process a batch of events",
    buckets=[0.5, 1, 2, 5, 10, 30, 60, 120, 300],
)

oldest_unprocessed = Gauge(
    "parser_oldest_unprocessed_seconds",
    "Age in seconds of the oldest unprocessed event",
)

llm_available = Gauge(
    "parser_llm_available",
    "Whether the LLM endpoint is currently reachable (1=up, 0=down)",
)


def start_metrics_server() -> None:
    """Start Prometheus metrics HTTP server on a background thread."""
    try:
        start_http_server(METRICS_PORT)
        logger.info("Prometheus metrics server started on port %d", METRICS_PORT)
    except OSError as e:
        logger.error("Failed to start metrics server: %s", e)
